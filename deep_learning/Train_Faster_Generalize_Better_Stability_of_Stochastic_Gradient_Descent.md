# Train faster, generalize better: Stability of stochastic gradient descent

> a stochastic gradient method (SGM) with few iterations have vanishing generalization error

## Introduction

Fast trainning time allows for the generation. Also, it is good for computational costs and decreasing the generation error. The paper gives the reason about the generalization of deep neural network. The notion of uniform stability[^UniformStability] is important.



The generalization error is the difference between empirical risk and population risk. Also, the proofs are checked in our course by Prof. Zhihua Zhang.



## footnote

[^UniformStability]: A concept put forward in paper [Stability and Generalization](http://www.jmlr.org/papers/volume2/bousquet02a/bousquet02a.pdf)